---
layout: default
title: "ProbNum25 : Keynotes"
header_image: Juan2.jpg
---
## Keynote 1 (September 1st, 10 am): 
- **Speaker**: [Samuel Kaski](https://kaski-lab.com/) (ELLIS Institute Finland, Aalto University and University of Manchester) 


---
## Keynote 2: (September 2nd, 10 am) 
- **Speaker**: [Arthur Gretton](https://www.gatsby.ucl.ac.uk/~gretton/) (Gatsby Unit, University College London and Google DeepMind)
- **Title**: Wasserstein Gradient Flow on the Maximum Mean Discrepancy
- **Abstract**: We construct a Wasserstein gradient flow on the Maximum Mean Discrepancy (MMD): an integral probability metric defined for a reproducing kernel Hilbert space (RKHS), which serves as a metric on probability measures for a sufficiently rich RKHS. This flow transports particles from an initial distribution to a target distribution, where the latter is provided simply as a sample, and can be used to generate new samples from the target distribution. We obtain conditions for convergence of the gradient flow towards a global optimum, and relate this flow to the problem of optimizing neural network parameters. We propose a way to regularize the MMD gradient flow, based on an injection of noise in the gradient, and give theoretical and empirical evidence for this procedure. We provide empirical validation of the MMD gradient flow in the setting of neural network training.

 ---
